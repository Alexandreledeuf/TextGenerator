{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd2f9e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today we want war exchequer what than any faster than a powerful vehicle not a face – rent me what than do all rent and blackbrowd clue – than do after what and what than see claimed was what sea she intellectual and as not intellectual a cause and had what than not intellectual a saw to dight not got and sea have not got and the almondblossom of them not us and when them our orders me not got and the almondblossom which is what is heart was sea sea have not perceiving to sea what is not a amends is not us and when them all life and sea it as a cause and as a cause and as a cause and as a cause and she a or is like a meal – not a clue – is the sea me as claimed not a meal part was what in our meal me life and what is a heart was rent it as a us and when them them all claimed not a face – and see claimed not a fairer part more minds more me not a fairer part more minds white part livid skin me was a repulsive part livid part more bodies like few small hospital part when aroused like cadets the battle drill and bombs again claimed was what and scarred found part me what and our fairer hand— it as what is their spacious swivel and not life and as claimed she a hands – like us and she a face – and see claimed not intellectual a cause and as claimed she a hands – like us for she intellectual me she a or is like intellectual a chance in all a cause and she a or as like intellectual me she a or is like intellectual a explores as she a or as not us a sweet as a us and she a or as not sea a explores me she a or is like intellectual a chance in all a cause sea she a or as like intellectual me she a or as sea was a repulsive part more out of few peace is like intellectual me she a hands as a button me she the face livid skin me what is the sea swivel and an meal part was what in our fairer swivel a us and as what is like intellectual me what is their guns swivel is the sea course meal more and not us and when them all life and among them it is a perceiving and as not a meal is not us and when them them life as among them all what and she a fairer part more minds the part livid skin and was claimed not us and when it was claimed not perceiving and as what is the sea me as claimed not us and when it was claimed not a amends is not a fairer part more minds more god a us and\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from keras.utils import np_utils\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Dropout, Bidirectional, GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#obtenir les lignes de chaque poèmes\n",
    "def ObtainTexts():\n",
    "    LigneTexte = []\n",
    "    for filename in os.listdir(\"war/\"):\n",
    "        dir = \"war/\"+str(filename)\n",
    "        file = open(dir, encoding=\"utf8\")\n",
    "        while(True):\n",
    "            texte = []\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            else:\n",
    "                LigneTexte.append(line)\n",
    "    return LigneTexte\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r',', '', text)\n",
    "    text = re.sub(r'\\'', '',  text)\n",
    "    text = re.sub(r'\\\"', '', text)\n",
    "    text = re.sub(r'\\(', '', text)\n",
    "    text = re.sub(r'\\)', '', text)\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(r'“', '', text)\n",
    "    text = re.sub(r'”', '', text)\n",
    "    text = re.sub(r'’', '', text)\n",
    "    text = re.sub(r'\\.', '', text)\n",
    "    text = re.sub(r';', '', text)\n",
    "    text = re.sub(r':', '', text)\n",
    "    text = re.sub(r'\\-', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def dataCleaning(LigneTexte):\n",
    "    #enlever les \\n\n",
    "    for i in range(len(LigneTexte)):\n",
    "        LigneTexte[i] = LigneTexte[i].replace('\\n', '')\n",
    "        LigneTexte[i] = LigneTexte[i].lower()\n",
    "        LigneTexte[i] = clean_text(LigneTexte[i])\n",
    "    return LigneTexte\n",
    "\n",
    "def tokenize(LigneTexte):\n",
    "   # Instantiating the Tokenizer\n",
    "    max_vocab = 1000000\n",
    "    tokenizer = Tokenizer(num_words=max_vocab)\n",
    "    tokenizer.fit_on_texts(LigneTexte)   \n",
    "\n",
    "    # Getting the total number of words of the data.\n",
    "    word2idx = tokenizer.word_index\n",
    "    vocab_size = len(word2idx) + 1 \n",
    "    return vocab_size, tokenizer\n",
    "\n",
    "# We will turn the sentences to sequences line by line and create n_gram sequences\n",
    "def sentToSeq(LigneTexte,tokenizer):\n",
    "    input_seq = []\n",
    "\n",
    "    for line in LigneTexte:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_seq = token_list[:i+1]\n",
    "            input_seq.append(n_gram_seq)\n",
    "    return input_seq\n",
    "\n",
    "# Getting the maximum length of sequence for padding purpose\n",
    "def maxLenSequence(input_seq):\n",
    "    return max(len(x) for x in input_seq)\n",
    "\n",
    "# Padding the sequences and converting them to array\n",
    "def padAndArray(input_seq,max_seq_length):\n",
    "    return np.array(pad_sequences(input_seq, maxlen=max_seq_length, padding='pre'))\n",
    "\n",
    "# Taking xs and labels to train the model.\n",
    "def XAndLabel(input_seq):\n",
    "    xs = input_seq[:, :-1]        # xs contains every word in sentence except the last one because we are using this value to predict the y value\n",
    "    labels = input_seq[:, -1]     # labels contains only the last word of the sentence which will help in hot encoding the y value in next step\n",
    "    return xs,labels\n",
    "\n",
    "# one-hot encoding the labels according to the vocab size\n",
    "\n",
    "# The matrix is square matrix of the size of vocab_size. Each row will denote a label and it will have \n",
    "# a single +ve value(i.e 1) for that label and other values will be zero. \n",
    "def categorical(labels,vocab_size):\n",
    "    return to_categorical(labels, num_classes=vocab_size)\n",
    "\n",
    "\n",
    "def textGeneratorModel(vocab_size,max_seq_length,xs,ys):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 124, input_length=max_seq_length-1))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(520, return_sequences=True))\n",
    "    model.add(Bidirectional(LSTM(340, return_sequences=True)))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=Adam(lr=0.001),loss = 'categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    r = model.fit(xs,ys,epochs=100)\n",
    "    model.save('modelWar.h5')\n",
    "    return r\n",
    "\n",
    "def DisplayAccuracy(r):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(r.history['accuracy'])\n",
    "    \n",
    "def predict_words(seed, no_words,model,tokenizer,maxLenInputSeq):\n",
    "    for i in range(no_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=maxLenInputSeq-1, padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list), axis=1)\n",
    "\n",
    "        new_word = ''\n",
    "\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if predicted == index:\n",
    "                new_word = word\n",
    "                break\n",
    "        seed += \" \" + new_word\n",
    "    print(seed)\n",
    "    \n",
    "def main():\n",
    "    LigneTexte = ObtainTexts()\n",
    "    LigneTexte = dataCleaning(LigneTexte)\n",
    "    vocab_size,tokenizer = tokenize(LigneTexte)\n",
    "    input_seq = sentToSeq(LigneTexte,tokenizer)\n",
    "    maxLenInputSeq = maxLenSequence(input_seq)\n",
    "    input_seq  = padAndArray(input_seq,maxLenInputSeq)\n",
    "    xs,labels = XAndLabel(input_seq)\n",
    "    ys = categorical(labels,vocab_size)\n",
    "    #r = textGeneratorModel(vocab_size,maxLenInputSeq,xs,ys)\n",
    "    #DisplayAccuracy(r)\n",
    "\n",
    "    \n",
    "    model=load_model('modelWar.h5') \n",
    "    seed_text = 'Today we want' #Début de la phrase, qui peut être changé mais seulement en Anglais\n",
    "    next_words = 500 #Nombre de mots que l'on veut générer\n",
    "    predict_words(seed_text, next_words,model,tokenizer,maxLenInputSeq)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d61df0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
